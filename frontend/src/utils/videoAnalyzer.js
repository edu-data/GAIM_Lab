/**
 * í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ë¹„ë””ì˜¤ ë¶„ì„ ì—”ì§„
 * Canvas + Web Audio APIë¥¼ í™œìš©í•œ ì‹¤ì œ ë¹„ë””ì˜¤ ë¶„ì„
 */

// â”€â”€ Extractor: ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ + ì˜¤ë””ì˜¤ ì¶”ì¶œ â”€â”€
export async function extractResources(videoFile, onProgress) {
    const url = URL.createObjectURL(videoFile)

    // ë¹„ë””ì˜¤ ë¡œë“œ (ë¨¼ì €!)
    const video = document.createElement('video')
    video.muted = true
    video.playsInline = true
    video.preload = 'auto'
    video.src = url

    await new Promise((resolve, reject) => {
        const done = () => { video.onloadeddata = null; video.onerror = null; resolve() }
        video.onloadeddata = done
        video.onerror = () => { video.onloadeddata = null; reject(new Error('ë¹„ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨')) }
        setTimeout(() => { video.onloadeddata = null; reject(new Error('ë¹„ë””ì˜¤ ë¡œë“œ ì‹œê°„ ì´ˆê³¼ (30ì´ˆ)')) }, 30000)
    })

    const duration = video.duration
    if (!duration || !isFinite(duration) || duration <= 0) {
        URL.revokeObjectURL(url)
        throw new Error('ë¹„ë””ì˜¤ ê¸¸ì´ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
    }

    if (onProgress) onProgress(5)

    const width = Math.min(video.videoWidth || 320, 320)
    const height = Math.round((video.videoHeight || 240) * (width / (video.videoWidth || 320)))
    const canvas = document.createElement('canvas')
    canvas.width = width
    canvas.height = height
    const ctx = canvas.getContext('2d', { willReadFrequently: true })

    // ë™ì  ìƒ˜í”Œë§: ìµœëŒ€ 60í”„ë ˆì„ê¹Œì§€ë§Œ
    const maxFrames = 60
    const interval = Math.max(1, Math.floor(duration / maxFrames))
    const totalFrames = Math.min(Math.floor(duration / interval), maxFrames)
    const frames = []

    for (let i = 0; i < totalFrames; i++) {
        const time = i * interval
        try {
            // seek
            await new Promise((resolve) => {
                const handler = () => { video.removeEventListener('seeked', handler); resolve() }
                video.addEventListener('seeked', handler)
                video.currentTime = time
                setTimeout(() => { video.removeEventListener('seeked', handler); resolve() }, 2000)
            })
            // UI ìŠ¤ë ˆë“œì— ì–‘ë³´ (ê²€ì • í™”ë©´ ë°©ì§€)
            await new Promise(r => setTimeout(r, 0))
            ctx.drawImage(video, 0, 0, width, height)
            const imageData = ctx.getImageData(0, 0, width, height)
            frames.push({ time, imageData, width, height })
        } catch (e) {
            console.warn(`Frame ${i} skip:`, e.message)
        }
        if (onProgress) onProgress(5 + Math.round((i / totalFrames) * 75))
    }

    // ì˜¤ë””ì˜¤: 50MB ë¯¸ë§Œì¼ ë•Œë§Œ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ë³´í˜¸)
    let audioData = null
    if (videoFile.size < 50 * 1024 * 1024) {
        try {
            if (onProgress) onProgress(85)
            const ab = await videoFile.arrayBuffer()
            const audioCtx = new (window.AudioContext || window.webkitAudioContext)()
            audioData = await audioCtx.decodeAudioData(ab)
            audioCtx.close()
        } catch (e) {
            console.warn('Audio decode skipped:', e.message)
        }
    } else {
        console.log('Audio skipped: file too large (' + (videoFile.size / 1024 / 1024).toFixed(0) + 'MB)')
    }

    URL.revokeObjectURL(url)
    if (onProgress) onProgress(100)

    if (frames.length === 0) {
        throw new Error('í”„ë ˆì„ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§€ì›ë˜ëŠ” ë¹„ë””ì˜¤ í˜•ì‹ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.')
    }

    return {
        frames,
        audioData,
        duration,
        width,
        height,
        totalFrames: frames.length,
        fps: 1 / interval,
        videoWidth: video.videoWidth || width,
        videoHeight: video.videoHeight || height,
    }
}

// â”€â”€ Vision Agent: ì›€ì§ì„ + ì œìŠ¤ì²˜ ë¶„ì„ â”€â”€
export function analyzeVision(frames, onProgress) {
    const movements = []
    let totalMovement = 0
    let highMovementCount = 0
    let lowMovementCount = 0

    for (let i = 1; i < frames.length; i++) {
        const prev = frames[i - 1].imageData.data
        const curr = frames[i].imageData.data
        let diff = 0
        const pixelCount = prev.length / 4

        for (let p = 0; p < prev.length; p += 16) { // ìƒ˜í”Œë§ (4px ê°„ê²©)
            diff += Math.abs(prev[p] - curr[p])       // R
            diff += Math.abs(prev[p + 1] - curr[p + 1]) // G
            diff += Math.abs(prev[p + 2] - curr[p + 2]) // B
        }

        const normalizedDiff = diff / (pixelCount / 4 * 3) // 0-255 ìŠ¤ì¼€ì¼
        const movementPercent = Math.min(100, (normalizedDiff / 30) * 100)
        movements.push({ time: frames[i].time, movement: movementPercent })
        totalMovement += movementPercent

        if (movementPercent > 40) highMovementCount++
        else if (movementPercent < 10) lowMovementCount++

        if (onProgress) onProgress(Math.round((i / frames.length) * 100))
    }

    const avgMovement = totalMovement / Math.max(1, movements.length)
    const gestureActivity = (highMovementCount / Math.max(1, movements.length)) * 100

    return {
        movements,
        avgMovement: +avgMovement.toFixed(1),
        gestureActivity: +gestureActivity.toFixed(1),
        highMovementFrames: highMovementCount,
        lowMovementFrames: lowMovementCount,
        totalFrames: frames.length,
        desc: `ì œìŠ¤ì²˜ í™œì„± ${gestureActivity.toFixed(1)}%, í‰ê·  ì›€ì§ì„ ${avgMovement.toFixed(1)}%`,
    }
}

// â”€â”€ Content Agent: ìŠ¬ë¼ì´ë“œ ë³€í™” ê°ì§€ â”€â”€
export function analyzeContent(frames, onProgress) {
    const slideChanges = []
    let currentSlideStart = 0
    const SLIDE_THRESHOLD = 25 // ìŠ¬ë¼ì´ë“œ ì „í™˜ ê°ì§€ ì„ê³„ê°’

    // í™”ë©´ ìƒë‹¨ 70%ë§Œ ë¶„ì„ (ìŠ¬ë¼ì´ë“œ ì˜ì—­)
    for (let i = 1; i < frames.length; i++) {
        const prev = frames[i - 1].imageData
        const curr = frames[i].imageData
        const w = prev.width
        const h = Math.floor(prev.height * 0.7)
        let diff = 0
        let count = 0

        for (let y = 0; y < h; y += 4) {
            for (let x = 0; x < w; x += 4) {
                const idx = (y * w + x) * 4
                const dr = Math.abs(prev.data[idx] - curr.data[idx])
                const dg = Math.abs(prev.data[idx + 1] - curr.data[idx + 1])
                const db = Math.abs(prev.data[idx + 2] - curr.data[idx + 2])
                diff += (dr + dg + db) / 3
                count++
            }
        }

        const avgDiff = diff / Math.max(1, count)
        if (avgDiff > SLIDE_THRESHOLD) {
            slideChanges.push({
                from: currentSlideStart,
                to: frames[i].time,
                duration: frames[i].time - currentSlideStart,
            })
            currentSlideStart = frames[i].time
        }

        if (onProgress) onProgress(Math.round((i / frames.length) * 100))
    }

    // ë§ˆì§€ë§‰ ìŠ¬ë¼ì´ë“œ
    const lastTime = frames[frames.length - 1]?.time || 0
    if (lastTime > currentSlideStart) {
        slideChanges.push({
            from: currentSlideStart,
            to: lastTime,
            duration: lastTime - currentSlideStart,
        })
    }

    // í”„ë ˆì„ ë°ê¸° ë¶„ì„ (í…ìŠ¤íŠ¸ ë°€ë„ ì¶”ì •)
    let totalBrightness = 0
    for (const frame of frames) {
        const d = frame.imageData.data
        let brightness = 0
        for (let i = 0; i < d.length; i += 16) {
            brightness += (d[i] * 0.299 + d[i + 1] * 0.587 + d[i + 2] * 0.114)
        }
        totalBrightness += brightness / (d.length / 16)
    }
    const avgBrightness = totalBrightness / frames.length
    const textDensity = Math.min(100, Math.round(avgBrightness / 2.55))

    return {
        slideChanges,
        slideCount: slideChanges.length,
        avgBrightness: +avgBrightness.toFixed(1),
        textDensity,
        desc: `ìŠ¬ë¼ì´ë“œ ${slideChanges.length}ì¥ ê°ì§€, í…ìŠ¤íŠ¸ ë°€ë„ ${textDensity}`,
    }
}

// â”€â”€ STT Agent (ì‹œë®¬ë ˆì´ì…˜): ì˜¤ë””ì˜¤ ê¸°ë°˜ ìŒì„± í™œë™ ê°ì§€ â”€â”€
export function analyzeSTT(audioData, onProgress) {
    if (!audioData) {
        if (onProgress) onProgress(100)
        return {
            speechActivity: 0,
            silenceRatio: 100,
            estimatedWords: 0,
            segments: [],
            desc: 'ì˜¤ë””ì˜¤ ë°ì´í„° ì—†ìŒ',
        }
    }

    const channelData = audioData.getChannelData(0)
    const sampleRate = audioData.sampleRate
    const segmentSize = Math.floor(sampleRate * 0.5) // 0.5ì´ˆ ì„¸ê·¸ë¨¼íŠ¸
    const segments = []
    let speechFrames = 0
    let silenceFrames = 0
    const totalSegments = Math.floor(channelData.length / segmentSize)

    for (let i = 0; i < totalSegments; i++) {
        const start = i * segmentSize
        const end = Math.min(start + segmentSize, channelData.length)
        let rms = 0
        for (let j = start; j < end; j++) {
            rms += channelData[j] * channelData[j]
        }
        rms = Math.sqrt(rms / (end - start))

        const isSpeech = rms > 0.02
        if (isSpeech) speechFrames++
        else silenceFrames++

        segments.push({
            time: +(i * 0.5).toFixed(1),
            rms: +rms.toFixed(4),
            isSpeech,
        })

        if (onProgress) onProgress(Math.round((i / totalSegments) * 100))
    }

    const total = speechFrames + silenceFrames
    const speechRatio = total > 0 ? (speechFrames / total) * 100 : 0
    const silenceRatio = 100 - speechRatio
    // ì¶”ì • ì–´ì ˆ: ìŒì„± í™œì„± ì‹œê°„ Ã— ë¶„ë‹¹ ì•½ 200ì–´ì ˆ / 60ì´ˆ Ã— 0.5ì´ˆ ì„¸ê·¸ë¨¼íŠ¸
    const estimatedWords = Math.round(speechFrames * 0.5 * (200 / 60))

    return {
        speechActivity: +speechRatio.toFixed(1),
        silenceRatio: +silenceRatio.toFixed(1),
        estimatedWords,
        segments,
        desc: `ìŒì„± í™œì„± ${speechRatio.toFixed(1)}%, ì¶”ì • ${estimatedWords}ì–´ì ˆ`,
    }
}

// â”€â”€ Vibe Agent: ì˜¤ë””ì˜¤ í”„ë¡œì†Œë””(ìš´ìœ¨) ë¶„ì„ â”€â”€
export function analyzeVibe(audioData, onProgress) {
    if (!audioData) {
        if (onProgress) onProgress(100)
        return {
            avgVolume: 0,
            volumeVariation: 0,
            silenceRatio: 0,
            energyTimeline: [],
            desc: 'ì˜¤ë””ì˜¤ ë°ì´í„° ì—†ìŒ',
        }
    }

    const channelData = audioData.getChannelData(0)
    const sampleRate = audioData.sampleRate
    const windowSize = Math.floor(sampleRate * 1.0) // 1ì´ˆ ìœˆë„ìš°
    const totalWindows = Math.floor(channelData.length / windowSize)
    const energyTimeline = []
    let totalRms = 0
    let silentWindows = 0

    for (let i = 0; i < totalWindows; i++) {
        const start = i * windowSize
        const end = Math.min(start + windowSize, channelData.length)
        let rms = 0
        for (let j = start; j < end; j++) {
            rms += channelData[j] * channelData[j]
        }
        rms = Math.sqrt(rms / (end - start))
        const dbLevel = rms > 0 ? 20 * Math.log10(rms) : -100

        energyTimeline.push({ time: i, rms: +rms.toFixed(4), db: +dbLevel.toFixed(1) })
        totalRms += rms
        if (rms < 0.015) silentWindows++

        if (onProgress) onProgress(Math.round((i / totalWindows) * 100))
    }

    const avgRms = totalRms / Math.max(1, totalWindows)
    let variation = 0
    for (const e of energyTimeline) {
        variation += Math.pow(e.rms - avgRms, 2)
    }
    variation = Math.sqrt(variation / Math.max(1, totalWindows))

    const avgVolume = +(avgRms * 1000).toFixed(1)
    const volumeVariation = +(variation * 1000).toFixed(2)
    const silenceRatio = +((silentWindows / Math.max(1, totalWindows)) * 100).toFixed(1)

    return {
        avgVolume,
        volumeVariation,
        silenceRatio,
        energyTimeline,
        desc: `ë³¼ë¥¨ ë³€ë™ ${volumeVariation}, ì¹¨ë¬µ ë¹„ìœ¨ ${silenceRatio}%`,
    }
}

// â”€â”€ Pedagogy Agent: 7ì°¨ì› êµìœ¡í•™ í‰ê°€ â”€â”€
export function evaluatePedagogy(visionResult, contentResult, sttResult, vibeResult, onProgress) {
    if (onProgress) onProgress(10)

    // 1. êµìˆ˜ ì „ë‹¬ë ¥ (ìŒì„± ê¸°ë°˜)
    const deliveryScore = Math.min(20, Math.round(
        (sttResult.speechActivity > 60 ? 12 : sttResult.speechActivity > 40 ? 8 : 4) +
        (vibeResult.volumeVariation > 2 ? 5 : vibeResult.volumeVariation > 1 ? 3 : 1) +
        (vibeResult.silenceRatio < 30 ? 3 : vibeResult.silenceRatio < 50 ? 1 : 0)
    ))
    if (onProgress) onProgress(25)

    // 2. ë¹„ì–¸ì–´ì  ì†Œí†µ (ì œìŠ¤ì²˜/ì›€ì§ì„)
    const nonverbalScore = Math.min(15, Math.round(
        (visionResult.gestureActivity > 30 ? 10 : visionResult.gestureActivity > 15 ? 7 : 3) +
        (visionResult.avgMovement > 20 ? 5 : visionResult.avgMovement > 10 ? 3 : 1)
    ))
    if (onProgress) onProgress(40)

    // 3. ìˆ˜ì—… êµ¬ì„± (ìŠ¬ë¼ì´ë“œ ë³€í™”)
    const structureScore = Math.min(15, Math.round(
        (contentResult.slideCount > 5 ? 10 : contentResult.slideCount > 2 ? 7 : 4) +
        (contentResult.textDensity > 50 ? 5 : contentResult.textDensity > 30 ? 3 : 1)
    ))
    if (onProgress) onProgress(55)

    // 4. í•™ìŠµ ìë£Œ í™œìš©
    const materialScore = Math.min(10, Math.round(
        (contentResult.slideCount > 3 ? 7 : contentResult.slideCount > 1 ? 5 : 2) +
        (contentResult.avgBrightness > 100 ? 3 : 1)
    ))
    if (onProgress) onProgress(70)

    // 5. ì‹œê°„ ê´€ë¦¬
    const timeScore = Math.min(10, Math.round(
        vibeResult.silenceRatio < 20 ? 8 :
            vibeResult.silenceRatio < 40 ? 6 :
                vibeResult.silenceRatio < 60 ? 4 : 2
    ))
    if (onProgress) onProgress(80)

    // 6. í•™ìŠµì ìƒí˜¸ì‘ìš© (ì¶”ë¡ )
    const interactionScore = Math.min(15, Math.round(
        (sttResult.speechActivity > 70 ? 5 : 3) +
        (visionResult.highMovementFrames > 10 ? 5 : 3) +
        (vibeResult.volumeVariation > 1.5 ? 5 : 3)
    ))
    if (onProgress) onProgress(90)

    // 7. ì „ë¬¸ì„±
    const expertiseScore = Math.min(15, Math.round(
        (sttResult.estimatedWords > 500 ? 8 : sttResult.estimatedWords > 200 ? 5 : 3) +
        (contentResult.slideCount > 3 ? 4 : 2) +
        (visionResult.gestureActivity > 20 ? 3 : 1)
    ))
    if (onProgress) onProgress(100)

    const dimensions = [
        { name: 'êµìˆ˜ ì „ë‹¬ë ¥', score: deliveryScore, max: 20 },
        { name: 'ë¹„ì–¸ì–´ì  ì†Œí†µ', score: nonverbalScore, max: 15 },
        { name: 'ìˆ˜ì—… êµ¬ì„±', score: structureScore, max: 15 },
        { name: 'í•™ìŠµ ìë£Œ í™œìš©', score: materialScore, max: 10 },
        { name: 'ì‹œê°„ ê´€ë¦¬', score: timeScore, max: 10 },
        { name: 'í•™ìŠµì ìƒí˜¸ì‘ìš©', score: interactionScore, max: 15 },
        { name: 'ì „ë¬¸ì„±', score: expertiseScore, max: 15 },
    ]

    const totalScore = dimensions.reduce((sum, d) => sum + d.score, 0)
    const grade = totalScore >= 90 ? 'A+' : totalScore >= 80 ? 'A' : totalScore >= 70 ? 'B+' :
        totalScore >= 60 ? 'B' : totalScore >= 50 ? 'C+' : totalScore >= 40 ? 'C' : 'D'

    return {
        dimensions,
        totalScore,
        grade,
        desc: `7ì°¨ì› í‰ê°€: ${totalScore}/100 (${grade})`,
    }
}

// â”€â”€ Feedback Agent: í”¼ë“œë°± ìƒì„± â”€â”€
export function generateFeedback(pedagogyResult, visionResult, vibeResult, onProgress) {
    if (onProgress) onProgress(20)

    const strengths = []
    const improvements = []

    for (const dim of pedagogyResult.dimensions) {
        const ratio = dim.score / dim.max
        if (ratio >= 0.7) {
            strengths.push({ dimension: dim.name, score: dim.score, max: dim.max, message: `${dim.name}ì´(ê°€) ìš°ìˆ˜í•©ë‹ˆë‹¤ (${dim.score}/${dim.max})` })
        } else if (ratio < 0.5) {
            improvements.push({ dimension: dim.name, score: dim.score, max: dim.max, message: `${dim.name} ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤ (${dim.score}/${dim.max})` })
        }
    }

    if (onProgress) onProgress(50)

    // êµ¬ì²´ì  í”¼ë“œë°±
    const tips = []
    if (visionResult.gestureActivity < 20) tips.push('ğŸ’¡ ì œìŠ¤ì²˜ë¥¼ ë” ì ê·¹ì ìœ¼ë¡œ í™œìš©í•´ë³´ì„¸ìš”.')
    if (visionResult.avgMovement < 10) tips.push('ğŸ’¡ êµì‹¤ ë‚´ ì´ë™ì„ ëŠ˜ë ¤ í•™ìƒê³¼ì˜ ê±°ë¦¬ê°ì„ ì¤„ì—¬ë³´ì„¸ìš”.')
    if (vibeResult.silenceRatio > 40) tips.push('ğŸ’¡ ì¹¨ë¬µ êµ¬ê°„ì´ ê¸¸ì–´ìš”. í•™ìƒ ì°¸ì—¬ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ì„ ì¶”ê°€í•´ë³´ì„¸ìš”.')
    if (vibeResult.volumeVariation < 1) tips.push('ğŸ’¡ ëª©ì†Œë¦¬ì— ì–µì–‘ ë³€í™”ë¥¼ ì£¼ë©´ í•™ìƒë“¤ì˜ ì§‘ì¤‘ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.')

    if (onProgress) onProgress(100)

    return {
        strengths,
        improvements,
        tips,
        totalFeedback: strengths.length + improvements.length + tips.length,
        desc: `ê°•ì  ${strengths.length}ê±´, ê°œì„ ì  ${improvements.length}ê±´, íŒ ${tips.length}ê±´`,
    }
}

// â”€â”€ Master Agent: ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± â”€â”€
export function generateReport(extractResult, visionResult, contentResult, sttResult, vibeResult, pedagogyResult, feedbackResult, onProgress) {
    if (onProgress) onProgress(30)

    const report = {
        summary: {
            duration: extractResult.duration,
            totalFrames: extractResult.totalFrames,
            resolution: `${extractResult.videoWidth}Ã—${extractResult.videoHeight}`,
            totalScore: pedagogyResult.totalScore,
            grade: pedagogyResult.grade,
        },
        dimensions: pedagogyResult.dimensions,
        metrics: {
            gestureActivity: visionResult.gestureActivity,
            avgMovement: visionResult.avgMovement,
            slideCount: contentResult.slideCount,
            speechActivity: sttResult.speechActivity,
            estimatedWords: sttResult.estimatedWords,
            silenceRatio: vibeResult.silenceRatio,
            volumeVariation: vibeResult.volumeVariation,
        },
        feedback: feedbackResult,
        timestamp: new Date().toISOString(),
    }

    if (onProgress) onProgress(100)

    return {
        report,
        desc: `ì¢…í•© ë¦¬í¬íŠ¸: ${pedagogyResult.totalScore}ì  (${pedagogyResult.grade})`,
    }
}
