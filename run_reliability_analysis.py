"""
GAIM Lab - ê²€ì‚¬-ì¬ê²€ì‚¬ ì‹ ë¢°ë„ ë¶„ì„
Test-Retest Reliability Analysis for 20251209_110545.mp4
"""

import json
import math
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parent

# 1ì°¨, 2ì°¨ ê²°ê³¼ ë¡œë“œ
with open(PROJECT_ROOT / "test_largest_result.json", encoding="utf-8") as f:
    test1 = json.load(f)
with open(PROJECT_ROOT / "test_largest_result_retest.json", encoding="utf-8") as f:
    test2 = json.load(f)

print("=" * 70)
print("ğŸ“Š GAIM Lab ê²€ì‚¬-ì¬ê²€ì‚¬ ì‹ ë¢°ë„ ë¶„ì„ (Test-Retest Reliability)")
print("   ì˜ìƒ: 20251209_110545.mp4 (1,653 MB - ìµœëŒ€ ìš©ëŸ‰)")
print("=" * 70)

# ================================================================
# 1. ì°¨ì›ë³„ ì ìˆ˜ ë¹„êµ
# ================================================================
print("\n" + "â”€" * 70)
print("1ï¸âƒ£  ì°¨ì›ë³„ ì ìˆ˜ ë¹„êµ")
print("â”€" * 70)
print(f"{'ì°¨ì›':<15} {'1ì°¨':>6} {'2ì°¨':>6} {'ì°¨ì´':>6} {'ì¼ì¹˜ìœ¨':>8}")
print("â”€" * 70)

dims1 = {d["name"]: d for d in test1["dimensions"]}
dims2 = {d["name"]: d for d in test2["dimensions"]}

dim_names = [d["name"] for d in test1["dimensions"]]
scores1 = []
scores2 = []
pct1_list = []
pct2_list = []
diffs = []

for name in dim_names:
    d1 = dims1[name]
    d2 = dims2[name]
    s1 = d1["score"]
    s2 = d2["score"]
    max_s = d1["max_score"]
    diff = abs(s2 - s1)
    agreement = (1 - diff / max_s) * 100
    
    scores1.append(s1)
    scores2.append(s2)
    pct1_list.append(d1["percentage"])
    pct2_list.append(d2["percentage"])
    diffs.append(diff)
    
    marker = "âœ…" if diff <= 1 else ("âš ï¸" if diff <= 3 else "âŒ")
    print(f"{name:<15} {s1:>6.1f} {s2:>6.1f} {s2-s1:>+6.1f} {agreement:>7.1f}% {marker}")

print("â”€" * 70)
print(f"{'ì´ì ':<15} {test1['total_score']:>6.1f} {test2['total_score']:>6.1f} {test2['total_score']-test1['total_score']:>+6.1f}")
print(f"{'ë“±ê¸‰':<15} {test1['grade']:>6} {test2['grade']:>6}")

# ================================================================
# 2. Pearson ìƒê´€ê³„ìˆ˜
# ================================================================
n = len(scores1)
mean1 = sum(scores1) / n
mean2 = sum(scores2) / n

cov = sum((s1 - mean1) * (s2 - mean2) for s1, s2 in zip(scores1, scores2)) / n
std1 = math.sqrt(sum((s - mean1) ** 2 for s in scores1) / n)
std2 = math.sqrt(sum((s - mean2) ** 2 for s in scores2) / n)

pearson_r = cov / (std1 * std2) if std1 > 0 and std2 > 0 else 0

print(f"\n\n" + "â”€" * 70)
print("2ï¸âƒ£  Pearson ìƒê´€ê³„ìˆ˜ (r)")
print("â”€" * 70)
print(f"   r = {pearson_r:.4f}")

if pearson_r >= 0.9:
    r_interp = "ë§¤ìš° ë†’ì€ ìƒê´€ (Excellent)"
elif pearson_r >= 0.7:
    r_interp = "ë†’ì€ ìƒê´€ (Good)"
elif pearson_r >= 0.5:
    r_interp = "ë³´í†µ ìƒê´€ (Moderate)"
else:
    r_interp = "ë‚®ì€ ìƒê´€ (Poor)"

print(f"   í•´ì„: {r_interp}")

# ================================================================
# 3. ICC (Intraclass Correlation Coefficient) - ICC(3,1) Two-way mixed, consistency
# ================================================================
# ICC ê³„ì‚°: Two-way random, single measures, absolute agreement
# Formula: ICC = (MSR - MSE) / (MSR + (k-1)*MSE + k*(MSC-MSE)/n)
# Simplified for k=2 raters (test-retest):

k = 2  # test, retest
subjects = list(zip(scores1, scores2))

# Overall mean
grand_mean = sum(scores1 + scores2) / (n * k)

# Mean per subject (row means)
row_means = [(s1 + s2) / 2 for s1, s2 in subjects]

# Mean per rater (column means)
col_means = [mean1, mean2]

# SS Between Subjects (SSR)
SSR = k * sum((rm - grand_mean) ** 2 for rm in row_means)

# SS Between Raters (SSC)
SSC = n * sum((cm - grand_mean) ** 2 for cm in col_means)

# SS Total
SST = sum((s - grand_mean) ** 2 for s in scores1) + sum((s - grand_mean) ** 2 for s in scores2)

# SS Error (Residual)
SSE = SST - SSR - SSC

# Mean Squares
MSR = SSR / (n - 1) if n > 1 else 0
MSC = SSC / (k - 1) if k > 1 else 0
MSE = SSE / ((n - 1) * (k - 1)) if (n - 1) * (k - 1) > 0 else 0

# ICC(2,1) - Two-way random, single measures, absolute agreement
icc_2_1 = (MSR - MSE) / (MSR + (k - 1) * MSE + k * (MSC - MSE) / n) if (MSR + (k - 1) * MSE + k * (MSC - MSE) / n) > 0 else 0

# ICC(3,1) - Two-way mixed, consistency
icc_3_1 = (MSR - MSE) / (MSR + (k - 1) * MSE) if (MSR + (k - 1) * MSE) > 0 else 0

print(f"\n\n" + "â”€" * 70)
print("3ï¸âƒ£  ICC (ê¸‰ë‚´ìƒê´€ê³„ìˆ˜, Intraclass Correlation Coefficient)")
print("â”€" * 70)
print(f"   ICC(2,1) ì ˆëŒ€ì  ì¼ì¹˜ë„: {icc_2_1:.4f}")
print(f"   ICC(3,1) ì¼ê´€ì„±:       {icc_3_1:.4f}")

if icc_3_1 >= 0.9:
    icc_interp = "ë§¤ìš° ìš°ìˆ˜ (Excellent, â‰¥0.90)"
elif icc_3_1 >= 0.75:
    icc_interp = "ìš°ìˆ˜ (Good, 0.75â€“0.89)"
elif icc_3_1 >= 0.5:
    icc_interp = "ë³´í†µ (Moderate, 0.50â€“0.74)"
else:
    icc_interp = "ë¯¸í¡ (Poor, <0.50)"

print(f"   í•´ì„: {icc_interp}")
print(f"   (Koo & Li, 2016 ê¸°ì¤€)")

# ================================================================
# 4. í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE) ë° RMSE
# ================================================================
mae = sum(diffs) / n
rmse = math.sqrt(sum(d ** 2 for d in diffs) / n)

# ì´ì  ê¸°ì¤€ MAE/RMSE
total_diff = abs(test2["total_score"] - test1["total_score"])

print(f"\n\n" + "â”€" * 70)
print("4ï¸âƒ£  ì˜¤ì°¨ ë¶„ì„")
print("â”€" * 70)
print(f"   ì°¨ì›ë³„ MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae:.2f}ì ")
print(f"   ì°¨ì›ë³„ RMSE (í‰ê· ì œê³±ê·¼ì˜¤ì°¨): {rmse:.2f}ì ")
print(f"   ì´ì  ì°¨ì´: {total_diff:.1f}ì  (100ì  ë§Œì  ê¸°ì¤€ {total_diff:.1f}%)")

# ================================================================
# 5. ì„¸ë¶€ ê¸°ì¤€ ë¹„êµ
# ================================================================
print(f"\n\n" + "â”€" * 70)
print("5ï¸âƒ£  ì„¸ë¶€ ê¸°ì¤€(criteria) ë¹„êµ")
print("â”€" * 70)
print(f"{'ì°¨ì›':<15} {'ê¸°ì¤€':<20} {'1ì°¨':>5} {'2ì°¨':>5} {'ì°¨ì´':>6}")
print("â”€" * 70)

criteria_diffs = []
for name in dim_names:
    c1 = dims1[name].get("criteria", {})
    c2 = dims2[name].get("criteria", {})
    for key in c1:
        v1 = c1[key]
        v2 = c2.get(key, 0)
        diff = abs(v2 - v1)
        criteria_diffs.append(diff)
        marker = "âœ…" if diff == 0 else ("âš ï¸" if diff <= 1 else "âŒ")
        print(f"{name:<15} {key:<20} {v1:>5} {v2:>5} {v2-v1:>+6} {marker}")

criteria_exact = sum(1 for d in criteria_diffs if d == 0)
criteria_total = len(criteria_diffs)
criteria_close = sum(1 for d in criteria_diffs if d <= 1)

print("â”€" * 70)
print(f"   ì™„ì „ ì¼ì¹˜: {criteria_exact}/{criteria_total}ê°œ ({criteria_exact/criteria_total*100:.1f}%)")
print(f"   Â±1ì  ì´ë‚´: {criteria_close}/{criteria_total}ê°œ ({criteria_close/criteria_total*100:.1f}%)")

# ================================================================
# 6. ë“±ê¸‰ ì¼ì¹˜ë„
# ================================================================
print(f"\n\n" + "â”€" * 70)
print("6ï¸âƒ£  ë“±ê¸‰ ì¼ì¹˜ë„")
print("â”€" * 70)
grade_match = test1["grade"] == test2["grade"]
print(f"   1ì°¨ ë“±ê¸‰: {test1['grade']}")
print(f"   2ì°¨ ë“±ê¸‰: {test2['grade']}")
print(f"   ë“±ê¸‰ ì¼ì¹˜: {'âœ… ì¼ì¹˜' if grade_match else 'âŒ ë¶ˆì¼ì¹˜'}")

# ê°•ì  ì˜ì—­ ì¼ì¹˜
print(f"\n   1ì°¨ ê°•ì : {', '.join(test1.get('strengths', []))}")
print(f"   2ì°¨ ê°•ì : {', '.join(test2.get('strengths', []))}")
s1_set = set(test1.get("strengths", []))
s2_set = set(test2.get("strengths", []))
print(f"   ê°•ì  ì¼ì¹˜: {'âœ… ì¼ì¹˜' if s1_set == s2_set else 'âš ï¸ ë¶€ë¶„ ë¶ˆì¼ì¹˜'}")

# ================================================================
# ì¢…í•© íŒì •
# ================================================================
print(f"\n\n" + "=" * 70)
print("ğŸ† ê²€ì‚¬-ì¬ê²€ì‚¬ ì‹ ë¢°ë„ ì¢…í•© íŒì •")
print("=" * 70)

reliability_score = 0
reliability_max = 5
notes = []

# 1) ICC ê¸°ì¤€
if icc_3_1 >= 0.75:
    reliability_score += 1
    notes.append(f"âœ… ICC(3,1) = {icc_3_1:.4f} â†’ ìš°ìˆ˜ ìˆ˜ì¤€")
else:
    notes.append(f"âš ï¸ ICC(3,1) = {icc_3_1:.4f} â†’ ê°œì„  í•„ìš”")

# 2) Pearson r
if pearson_r >= 0.7:
    reliability_score += 1
    notes.append(f"âœ… Pearson r = {pearson_r:.4f} â†’ ë†’ì€ ìƒê´€")
else:
    notes.append(f"âš ï¸ Pearson r = {pearson_r:.4f} â†’ ê°œì„  í•„ìš”")

# 3) ì´ì  ì°¨ì´
if total_diff <= 5:
    reliability_score += 1
    notes.append(f"âœ… ì´ì  ì°¨ì´ = {total_diff:.1f}ì  â†’ í—ˆìš© ë²”ìœ„")
else:
    notes.append(f"âš ï¸ ì´ì  ì°¨ì´ = {total_diff:.1f}ì  â†’ í¼")

# 4) ë“±ê¸‰ ì¼ì¹˜
if grade_match:
    reliability_score += 1
    notes.append(f"âœ… ë“±ê¸‰ ì¼ì¹˜ ({test1['grade']})")
else:
    notes.append(f"âš ï¸ ë“±ê¸‰ ë¶ˆì¼ì¹˜ ({test1['grade']} â†’ {test2['grade']})")

# 5) ì„¸ë¶€ ê¸°ì¤€ ì¼ì¹˜
if criteria_close / criteria_total >= 0.8:
    reliability_score += 1
    notes.append(f"âœ… ì„¸ë¶€ê¸°ì¤€ Â±1ì  ì´ë‚´ {criteria_close/criteria_total*100:.0f}%")
else:
    notes.append(f"âš ï¸ ì„¸ë¶€ê¸°ì¤€ Â±1ì  ì´ë‚´ {criteria_close/criteria_total*100:.0f}%")

for note in notes:
    print(f"   {note}")

print(f"\n   ğŸ“Š ì‹ ë¢°ë„ ì ìˆ˜: {reliability_score}/{reliability_max}")

if reliability_score >= 4:
    verdict = "ğŸŸ¢ ë†’ì€ ì‹ ë¢°ë„ (High Reliability)"
elif reliability_score >= 3:
    verdict = "ğŸŸ¡ ì–‘í˜¸í•œ ì‹ ë¢°ë„ (Acceptable Reliability)"
else:
    verdict = "ğŸ”´ ë‚®ì€ ì‹ ë¢°ë„ (Low Reliability)"

print(f"   ğŸ† ì¢…í•© íŒì •: {verdict}")

# ================================================================
# JSON ê²°ê³¼ ì €ì¥
# ================================================================
reliability_result = {
    "video": "20251209_110545.mp4",
    "file_size_mb": 1653.42,
    "test1_score": test1["total_score"],
    "test1_grade": test1["grade"],
    "test2_score": test2["total_score"],
    "test2_grade": test2["grade"],
    "score_difference": test2["total_score"] - test1["total_score"],
    "grade_match": grade_match,
    "pearson_r": round(pearson_r, 4),
    "icc_2_1": round(icc_2_1, 4),
    "icc_3_1": round(icc_3_1, 4),
    "mae": round(mae, 2),
    "rmse": round(rmse, 2),
    "criteria_exact_match_rate": round(criteria_exact / criteria_total * 100, 1),
    "criteria_within_1pt_rate": round(criteria_close / criteria_total * 100, 1),
    "reliability_score": f"{reliability_score}/{reliability_max}",
    "verdict": verdict,
    "dimensions_comparison": [
        {
            "name": name,
            "test1_score": dims1[name]["score"],
            "test2_score": dims2[name]["score"],
            "difference": dims2[name]["score"] - dims1[name]["score"],
            "max_score": dims1[name]["max_score"]
        }
        for name in dim_names
    ]
}

result_path = PROJECT_ROOT / "test_retest_reliability.json"
with open(result_path, "w", encoding="utf-8") as f:
    json.dump(reliability_result, f, ensure_ascii=False, indent=2)
print(f"\nğŸ“„ ì‹ ë¢°ë„ ë¶„ì„ ê²°ê³¼: {result_path}")
